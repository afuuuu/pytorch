{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "np.random.seed(7) # 用來確保每次生成的隨機資料是一樣的，否則訓練結果無法比較\n",
    "x = np.random.rand(100, 1) ##vs. np.random.randn #一維陣列\n",
    "\n",
    "y = 2 + 5 * x + .2 * np.random.randn(100, 1) # randn 的 n 為 normal distribution\n",
    "\n",
    "idx = np.arange(100)\n",
    "np.random.shuffle(idx) # 打散索引\n",
    "\n",
    "train_idx = idx[:80] # 取前 80 筆為訓練資料\n",
    "val_idx = idx[80:] #取後 20 筆為驗證資料\n",
    "\n",
    "x_train, y_train = x[train_idx], y[train_idx]\n",
    "x_val, y_val = x[val_idx], y[val_idx]\n",
    "\n",
    "plt.scatter(x_train, y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "首先我們先隨便選參數 a, b 的初始值和設定基本的超參數 (hyper-parameter)，像是學習率，epoch 數等等："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 隨機給定數值，初始化 a, b 的值\n",
    "np.random.seed(7)\n",
    "a = np.random.randn(1)\n",
    "b = np.random.randn(1)\n",
    "# 設定學習率\n",
    "lr = 1e-1\n",
    "# 設定 epochs\n",
    "epochs = 500\n",
    "\n",
    "for epoch in range(epochs): \n",
    "    # 計算模型的預測 \n",
    "    yhat = a + b * x_train\n",
    "    # 用預測和標記來計算 error \n",
    "    error = (y_train - yhat)\n",
    "    # 用 error 來計算 loss\n",
    "    loss = (error ** 2).mean()\n",
    "    \n",
    "    # 計算兩個參數的梯度\n",
    "    a_grad = -2 * error.mean()\n",
    "    b_grad = -2 * (x_train * error).mean()\n",
    "    \n",
    "    # 用梯度和學習率來更新參數\n",
    "    a = a - lr * a_grad\n",
    "    b = b - lr * b_grad\n",
    "\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "LR = LinearRegression()\n",
    "LR.fit(x_train, y_train)\n",
    "print(LR.intercept_, LR.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'torch.Tensor'> torch.DoubleTensor\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "x_train_tensor = torch.from_numpy(x_train).to(device)\n",
    "y_train_tensor = torch.from_numpy(y_train).to(device)\n",
    "\n",
    "print(type(x_train), type(x_train_tensor), x_train_tensor.type())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor.type() 檢查 tensor 存放在哪   會得到 torch.cuda.DoubleTensor, 沒 gpu 則是 torch.DoubleTensor。\n",
    "\n",
    "當然我們也可以把 tensor 轉回去 numpy array，不過假如我們的 tensor 是在 gpu 上，我們需要先把他轉存到 cpu。\n",
    "x_train_tensor.cpu().numpy()\n",
    "\n",
    "注意 requires_grad 後面的底線「 _ 」，在 PyTorch 中，用「 _ 」結尾的方法都會改變變數本身。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(7)\n",
    "a = torch.randn(1, requires_grad=True, device=device)\n",
    "b = torch.randn(1, requires_grad=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "從下方開始，我們利用pytorch幫我們解決許多手刻的問題"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autograd\n",
    "backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-8.3123])\n",
      "tensor([-6.1476])\n",
      "tensor([-4.5359])\n",
      "tensor([-3.3360])\n",
      "tensor([-2.4428])\n",
      "tensor([-1.7780])\n",
      "tensor([-1.2833])\n",
      "tensor([-0.9153])\n",
      "tensor([-0.6417])\n",
      "tensor([-0.4383])\n",
      "tensor([-0.2872])\n",
      "tensor([-0.1751])\n",
      "tensor([-0.0919])\n",
      "tensor([-0.0304])\n",
      "tensor([0.0150])\n",
      "tensor([0.0484])\n",
      "tensor([0.0729])\n",
      "tensor([0.0908])\n",
      "tensor([0.1038])\n",
      "tensor([0.1130])\n",
      "tensor([0.1196])\n",
      "tensor([0.1241])\n",
      "tensor([0.1271])\n",
      "tensor([0.1290])\n",
      "tensor([0.1300])\n",
      "tensor([0.1305])\n",
      "tensor([0.1304])\n",
      "tensor([0.1301])\n",
      "tensor([0.1295])\n",
      "tensor([0.1287])\n",
      "tensor([0.1278])\n",
      "tensor([0.1268])\n",
      "tensor([0.1257])\n",
      "tensor([0.1246])\n",
      "tensor([0.1234])\n",
      "tensor([0.1223])\n",
      "tensor([0.1211])\n",
      "tensor([0.1199])\n",
      "tensor([0.1187])\n",
      "tensor([0.1175])\n",
      "tensor([0.1164])\n",
      "tensor([0.1152])\n",
      "tensor([0.1140])\n",
      "tensor([0.1129])\n",
      "tensor([0.1117])\n",
      "tensor([0.1106])\n",
      "tensor([0.1094])\n",
      "tensor([0.1083])\n",
      "tensor([0.1072])\n",
      "tensor([0.1061])\n",
      "tensor([0.1050])\n",
      "tensor([0.1040])\n",
      "tensor([0.1029])\n",
      "tensor([0.1019])\n",
      "tensor([0.1008])\n",
      "tensor([0.0998])\n",
      "tensor([0.0988])\n",
      "tensor([0.0978])\n",
      "tensor([0.0968])\n",
      "tensor([0.0958])\n",
      "tensor([0.0948])\n",
      "tensor([0.0938])\n",
      "tensor([0.0929])\n",
      "tensor([0.0919])\n",
      "tensor([0.0910])\n",
      "tensor([0.0900])\n",
      "tensor([0.0891])\n",
      "tensor([0.0882])\n",
      "tensor([0.0873])\n",
      "tensor([0.0864])\n",
      "tensor([0.0855])\n",
      "tensor([0.0847])\n",
      "tensor([0.0838])\n",
      "tensor([0.0829])\n",
      "tensor([0.0821])\n",
      "tensor([0.0813])\n",
      "tensor([0.0804])\n",
      "tensor([0.0796])\n",
      "tensor([0.0788])\n",
      "tensor([0.0780])\n",
      "tensor([0.0772])\n",
      "tensor([0.0764])\n",
      "tensor([0.0756])\n",
      "tensor([0.0748])\n",
      "tensor([0.0741])\n",
      "tensor([0.0733])\n",
      "tensor([0.0726])\n",
      "tensor([0.0718])\n",
      "tensor([0.0711])\n",
      "tensor([0.0704])\n",
      "tensor([0.0696])\n",
      "tensor([0.0689])\n",
      "tensor([0.0682])\n",
      "tensor([0.0675])\n",
      "tensor([0.0668])\n",
      "tensor([0.0662])\n",
      "tensor([0.0655])\n",
      "tensor([0.0648])\n",
      "tensor([0.0642])\n",
      "tensor([0.0635])\n",
      "tensor([0.0628])\n",
      "tensor([0.0622])\n",
      "tensor([0.0616])\n",
      "tensor([0.0609])\n",
      "tensor([0.0603])\n",
      "tensor([0.0597])\n",
      "tensor([0.0591])\n",
      "tensor([0.0585])\n",
      "tensor([0.0579])\n",
      "tensor([0.0573])\n",
      "tensor([0.0567])\n",
      "tensor([0.0561])\n",
      "tensor([0.0556])\n",
      "tensor([0.0550])\n",
      "tensor([0.0544])\n",
      "tensor([0.0539])\n",
      "tensor([0.0533])\n",
      "tensor([0.0528])\n",
      "tensor([0.0522])\n",
      "tensor([0.0517])\n",
      "tensor([0.0512])\n",
      "tensor([0.0507])\n",
      "tensor([0.0501])\n",
      "tensor([0.0496])\n",
      "tensor([0.0491])\n",
      "tensor([0.0486])\n",
      "tensor([0.0481])\n",
      "tensor([0.0476])\n",
      "tensor([0.0471])\n",
      "tensor([0.0467])\n",
      "tensor([0.0462])\n",
      "tensor([0.0457])\n",
      "tensor([0.0452])\n",
      "tensor([0.0448])\n",
      "tensor([0.0443])\n",
      "tensor([0.0439])\n",
      "tensor([0.0434])\n",
      "tensor([0.0430])\n",
      "tensor([0.0425])\n",
      "tensor([0.0421])\n",
      "tensor([0.0417])\n",
      "tensor([0.0412])\n",
      "tensor([0.0408])\n",
      "tensor([0.0404])\n",
      "tensor([0.0400])\n",
      "tensor([0.0396])\n",
      "tensor([0.0392])\n",
      "tensor([0.0388])\n",
      "tensor([0.0384])\n",
      "tensor([0.0380])\n",
      "tensor([0.0376])\n",
      "tensor([0.0372])\n",
      "tensor([0.0368])\n",
      "tensor([0.0365])\n",
      "tensor([0.0361])\n",
      "tensor([0.0357])\n",
      "tensor([0.0354])\n",
      "tensor([0.0350])\n",
      "tensor([0.0346])\n",
      "tensor([0.0343])\n",
      "tensor([0.0339])\n",
      "tensor([0.0336])\n",
      "tensor([0.0332])\n",
      "tensor([0.0329])\n",
      "tensor([0.0326])\n",
      "tensor([0.0322])\n",
      "tensor([0.0319])\n",
      "tensor([0.0316])\n",
      "tensor([0.0313])\n",
      "tensor([0.0309])\n",
      "tensor([0.0306])\n",
      "tensor([0.0303])\n",
      "tensor([0.0300])\n",
      "tensor([0.0297])\n",
      "tensor([0.0294])\n",
      "tensor([0.0291])\n",
      "tensor([0.0288])\n",
      "tensor([0.0285])\n",
      "tensor([0.0282])\n",
      "tensor([0.0279])\n",
      "tensor([0.0276])\n",
      "tensor([0.0273])\n",
      "tensor([0.0271])\n",
      "tensor([0.0268])\n",
      "tensor([0.0265])\n",
      "tensor([0.0262])\n",
      "tensor([0.0260])\n",
      "tensor([0.0257])\n",
      "tensor([0.0254])\n",
      "tensor([0.0252])\n",
      "tensor([0.0249])\n",
      "tensor([0.0247])\n",
      "tensor([0.0244])\n",
      "tensor([0.0242])\n",
      "tensor([0.0239])\n",
      "tensor([0.0237])\n",
      "tensor([0.0234])\n",
      "tensor([0.0232])\n",
      "tensor([0.0230])\n",
      "tensor([0.0227])\n",
      "tensor([0.0225])\n",
      "tensor([0.0223])\n",
      "tensor([0.0220])\n",
      "tensor([0.0218])\n",
      "tensor([0.0216])\n",
      "tensor([0.0214])\n",
      "tensor([0.0212])\n",
      "tensor([0.0209])\n",
      "tensor([0.0207])\n",
      "tensor([0.0205])\n",
      "tensor([0.0203])\n",
      "tensor([0.0201])\n",
      "tensor([0.0199])\n",
      "tensor([0.0197])\n",
      "tensor([0.0195])\n",
      "tensor([0.0193])\n",
      "tensor([0.0191])\n",
      "tensor([0.0189])\n",
      "tensor([0.0187])\n",
      "tensor([0.0185])\n",
      "tensor([0.0183])\n",
      "tensor([0.0181])\n",
      "tensor([0.0179])\n",
      "tensor([0.0178])\n",
      "tensor([0.0176])\n",
      "tensor([0.0174])\n",
      "tensor([0.0172])\n",
      "tensor([0.0170])\n",
      "tensor([0.0169])\n",
      "tensor([0.0167])\n",
      "tensor([0.0165])\n",
      "tensor([0.0164])\n",
      "tensor([0.0162])\n",
      "tensor([0.0160])\n",
      "tensor([0.0159])\n",
      "tensor([0.0157])\n",
      "tensor([0.0155])\n",
      "tensor([0.0154])\n",
      "tensor([0.0152])\n",
      "tensor([0.0151])\n",
      "tensor([0.0149])\n",
      "tensor([0.0148])\n",
      "tensor([0.0146])\n",
      "tensor([0.0145])\n",
      "tensor([0.0143])\n",
      "tensor([0.0142])\n",
      "tensor([0.0140])\n",
      "tensor([0.0139])\n",
      "tensor([0.0137])\n",
      "tensor([0.0136])\n",
      "tensor([0.0135])\n",
      "tensor([0.0133])\n",
      "tensor([0.0132])\n",
      "tensor([0.0131])\n",
      "tensor([0.0129])\n",
      "tensor([0.0128])\n",
      "tensor([0.0127])\n",
      "tensor([0.0125])\n",
      "tensor([0.0124])\n",
      "tensor([0.0123])\n",
      "tensor([0.0121])\n",
      "tensor([0.0120])\n",
      "tensor([0.0119])\n",
      "tensor([0.0118])\n",
      "tensor([0.0117])\n",
      "tensor([0.0115])\n",
      "tensor([0.0114])\n",
      "tensor([0.0113])\n",
      "tensor([0.0112])\n",
      "tensor([0.0111])\n",
      "tensor([0.0110])\n",
      "tensor([0.0108])\n",
      "tensor([0.0107])\n",
      "tensor([0.0106])\n",
      "tensor([0.0105])\n",
      "tensor([0.0104])\n",
      "tensor([0.0103])\n",
      "tensor([0.0102])\n",
      "tensor([0.0101])\n",
      "tensor([0.0100])\n",
      "tensor([0.0099])\n",
      "tensor([0.0098])\n",
      "tensor([0.0097])\n",
      "tensor([0.0096])\n",
      "tensor([0.0095])\n",
      "tensor([0.0094])\n",
      "tensor([0.0093])\n",
      "tensor([0.0092])\n",
      "tensor([0.0091])\n",
      "tensor([0.0090])\n",
      "tensor([0.0089])\n",
      "tensor([0.0088])\n",
      "tensor([0.0087])\n",
      "tensor([0.0087])\n",
      "tensor([0.0086])\n",
      "tensor([0.0085])\n",
      "tensor([0.0084])\n",
      "tensor([0.0083])\n",
      "tensor([0.0082])\n",
      "tensor([0.0081])\n",
      "tensor([0.0081])\n",
      "tensor([0.0080])\n",
      "tensor([0.0079])\n",
      "tensor([0.0078])\n",
      "tensor([0.0077])\n",
      "tensor([0.0076])\n",
      "tensor([0.0076])\n",
      "tensor([0.0075])\n",
      "tensor([0.0074])\n",
      "tensor([0.0073])\n",
      "tensor([0.0073])\n",
      "tensor([0.0072])\n",
      "tensor([0.0071])\n",
      "tensor([0.0070])\n",
      "tensor([0.0070])\n",
      "tensor([0.0069])\n",
      "tensor([0.0068])\n",
      "tensor([0.0068])\n",
      "tensor([0.0067])\n",
      "tensor([0.0066])\n",
      "tensor([0.0066])\n",
      "tensor([0.0065])\n",
      "tensor([0.0064])\n",
      "tensor([0.0064])\n",
      "tensor([0.0063])\n",
      "tensor([0.0062])\n",
      "tensor([0.0062])\n",
      "tensor([0.0061])\n",
      "tensor([0.0060])\n",
      "tensor([0.0060])\n",
      "tensor([0.0059])\n",
      "tensor([0.0059])\n",
      "tensor([0.0058])\n",
      "tensor([0.0057])\n",
      "tensor([0.0057])\n",
      "tensor([0.0056])\n",
      "tensor([0.0056])\n",
      "tensor([0.0055])\n",
      "tensor([0.0054])\n",
      "tensor([0.0054])\n",
      "tensor([0.0053])\n",
      "tensor([0.0053])\n",
      "tensor([0.0052])\n",
      "tensor([0.0052])\n",
      "tensor([0.0051])\n",
      "tensor([0.0051])\n",
      "tensor([0.0050])\n",
      "tensor([0.0050])\n",
      "tensor([0.0049])\n",
      "tensor([0.0049])\n",
      "tensor([0.0048])\n",
      "tensor([0.0048])\n",
      "tensor([0.0047])\n",
      "tensor([0.0047])\n",
      "tensor([0.0046])\n",
      "tensor([0.0046])\n",
      "tensor([0.0045])\n",
      "tensor([0.0045])\n",
      "tensor([0.0044])\n",
      "tensor([0.0044])\n",
      "tensor([0.0043])\n",
      "tensor([0.0043])\n",
      "tensor([0.0043])\n",
      "tensor([0.0042])\n",
      "tensor([0.0042])\n",
      "tensor([0.0041])\n",
      "tensor([0.0041])\n",
      "tensor([0.0040])\n",
      "tensor([0.0040])\n",
      "tensor([0.0040])\n",
      "tensor([0.0039])\n",
      "tensor([0.0039])\n",
      "tensor([0.0038])\n",
      "tensor([0.0038])\n",
      "tensor([0.0038])\n",
      "tensor([0.0037])\n",
      "tensor([0.0037])\n",
      "tensor([0.0037])\n",
      "tensor([0.0036])\n",
      "tensor([0.0036])\n",
      "tensor([0.0035])\n",
      "tensor([0.0035])\n",
      "tensor([0.0035])\n",
      "tensor([0.0034])\n",
      "tensor([0.0034])\n",
      "tensor([0.0034])\n",
      "tensor([0.0033])\n",
      "tensor([0.0033])\n",
      "tensor([0.0033])\n",
      "tensor([0.0032])\n",
      "tensor([0.0032])\n",
      "tensor([0.0032])\n",
      "tensor([0.0031])\n",
      "tensor([0.0031])\n",
      "tensor([0.0031])\n",
      "tensor([0.0030])\n",
      "tensor([0.0030])\n",
      "tensor([0.0030])\n",
      "tensor([0.0029])\n",
      "tensor([0.0029])\n",
      "tensor([0.0029])\n",
      "tensor([0.0029])\n",
      "tensor([0.0028])\n",
      "tensor([0.0028])\n",
      "tensor([0.0028])\n",
      "tensor([0.0027])\n",
      "tensor([0.0027])\n",
      "tensor([0.0027])\n",
      "tensor([0.0027])\n",
      "tensor([0.0026])\n",
      "tensor([0.0026])\n",
      "tensor([0.0026])\n",
      "tensor([0.0025])\n",
      "tensor([0.0025])\n",
      "tensor([0.0025])\n",
      "tensor([0.0025])\n",
      "tensor([0.0024])\n",
      "tensor([0.0024])\n",
      "tensor([0.0024])\n",
      "tensor([0.0024])\n",
      "tensor([0.0023])\n",
      "tensor([0.0023])\n",
      "tensor([0.0023])\n",
      "tensor([0.0023])\n",
      "tensor([0.0023])\n",
      "tensor([0.0022])\n",
      "tensor([0.0022])\n",
      "tensor([0.0022])\n",
      "tensor([0.0022])\n",
      "tensor([0.0021])\n",
      "tensor([0.0021])\n",
      "tensor([0.0021])\n",
      "tensor([0.0021])\n",
      "tensor([0.0021])\n",
      "tensor([0.0020])\n",
      "tensor([0.0020])\n",
      "tensor([0.0020])\n",
      "tensor([0.0020])\n",
      "tensor([0.0020])\n",
      "tensor([0.0019])\n",
      "tensor([0.0019])\n",
      "tensor([0.0019])\n",
      "tensor([0.0019])\n",
      "tensor([0.0019])\n",
      "tensor([0.0018])\n",
      "tensor([0.0018])\n",
      "tensor([0.0018])\n",
      "tensor([0.0018])\n",
      "tensor([0.0018])\n",
      "tensor([0.0017])\n",
      "tensor([0.0017])\n",
      "tensor([0.0017])\n",
      "tensor([0.0017])\n",
      "tensor([0.0017])\n",
      "tensor([0.0017])\n",
      "tensor([0.0016])\n",
      "tensor([0.0016])\n",
      "tensor([0.0016])\n",
      "tensor([0.0016])\n",
      "tensor([0.0016])\n",
      "tensor([0.0016])\n",
      "tensor([0.0015])\n",
      "tensor([0.0015])\n",
      "tensor([0.0015])\n",
      "tensor([0.0015])\n",
      "tensor([0.0015])\n",
      "tensor([0.0015])\n",
      "tensor([0.0014])\n",
      "tensor([0.0014])\n",
      "tensor([0.0014])\n",
      "tensor([0.0014])\n",
      "tensor([0.0014])\n",
      "tensor([0.0014])\n",
      "tensor([0.0014])\n",
      "tensor([0.0013])\n",
      "tensor([0.0013])\n",
      "tensor([0.0013])\n",
      "tensor([0.0013])\n",
      "tensor([0.0013])\n",
      "tensor([0.0013])\n",
      "tensor([0.0013])\n",
      "tensor([0.0013])\n",
      "tensor([0.0012])\n",
      "tensor([0.0012])\n",
      "tensor([0.0012])\n",
      "tensor([0.0012])\n",
      "tensor([0.0012])\n",
      "tensor([0.0012])\n",
      "tensor([0.0012])\n",
      "tensor([0.0012])\n",
      "tensor([0.0011])\n",
      "tensor([0.0011])\n",
      "tensor([0.0011])\n",
      "tensor([0.0011])\n",
      "tensor([0.0011])\n",
      "tensor([0.0011])\n",
      "tensor([0.0011])\n",
      "tensor([0.0011])\n",
      "tensor([0.0011])\n",
      "tensor([0.0010])\n",
      "tensor([1.9544], requires_grad=True) tensor([5.0883], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lr = 1e-1\n",
    "epochs = 500\n",
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, device=device)\n",
    "b = torch.randn(1, requires_grad=True, device=device)\n",
    "for epoch in range(epochs):\n",
    "    yhat = a + b * x_train_tensor\n",
    "    error = y_train_tensor - yhat\n",
    "    loss = (error ** 2).mean()\n",
    "\n",
    "#-----------------------------------------   \n",
    "\n",
    "    # 不用再自己計算梯度了 \n",
    "    # a_grad = -2 * error.mean()\n",
    "    # b_grad = -2 * (x_tensor * error).mean()\n",
    "    \n",
    "    # 從 loss 做 backward 來幫我們取得梯度\n",
    "    loss.backward()  ##可以得出 各參數的grad e.g: a_grad、b_grad\n",
    "#-----------------------------------------\n",
    "\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        a -= lr * a.grad\n",
    "        b -= lr * b.grad\n",
    "    print(a.grad)\n",
    "    \n",
    "    \n",
    "    # 清除梯度\n",
    "    a.grad.zero_()       ##是「梯度(a.grad)」清為零 而不是 「參數(a)」 \n",
    "    b.grad.zero_()       ##理由是 : 下方\n",
    "    \n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "梯度累加:\n",
    "    在 PyTorch 中，梯度會不斷往上加而不是替換掉，但是累加梯度的話會讓我們每次的梯度有偏差而影響方向，所以每次我們要用梯度去更新參數之後，需要用 zero_() 將梯度清零，那你一定會好奇為何不直接替代就好，因為在訓練 RNN 模型時，累積梯度就很方便。\n",
    "Backward(): \n",
    "    由後往前去找loss Func.\n",
    "with torch.no_grad():\n",
    "不用，梯度就會消失， .grad 就會變成 NoneType \n",
    "torch.no_grad() impacts the autograd engine and deactivate it. It will reduce memory usage and speed up computations but you won’t be able to backprop (which you don’t want in an eval script).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 優化器（Optimizer）\n",
    "目前為止我們都是利用梯度，手動地來更新參數。更新兩個可能不是什麼大問題，但是在實務上我們可能會有幾萬甚至幾億個參數需要更新，這時候我們就需要優化器了（Optimizer）\n",
    "step() :　做參數更新\n",
    "zero_grad() : 梯度清為零"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch import optim\n",
    "\n",
    "torch.manual_seed(7)\n",
    "a = torch.randn(1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "print(a, b)\n",
    "lr = 1e-1\n",
    "epochs = 500\n",
    "\n",
    "# 指定我們的優化器為 SGD\n",
    "optimizer = optim.SGD([a, b], lr=lr)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    yhat = a + b * x_train_tensor\n",
    "    error = y_train_tensor - yhat\n",
    "    loss = (error ** 2).mean()\n",
    "    \n",
    "    \n",
    "    # 不用再自己計算梯度了 \n",
    "    # a_grad = -2 * error.mean()\n",
    "    # b_grad = -2 * (x_tensor * error).mean()\n",
    "    # 從 loss 做 backward 來幫我們取得梯度\n",
    "    loss.backward()    \n",
    "    \n",
    "#-----------------------------------------    \n",
    "    # 不用手動更新梯度\n",
    "    # a -= lr * a.grad\n",
    "    # b -= lr * b.grad\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 不用自己將梯度清零\n",
    "    # a.grad.zero_()\n",
    "    # b.grad.zero_()\n",
    "    optimizer.zero_grad()\n",
    "#-----------------------------------------\n",
    "\n",
    "    \n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss\n",
    "我們一開始提到的 MSE 只是其中一種 loss function，像在分類問題，cross entropy 就是比較常見的 loss function。\n",
    "接下來示範如何在 PyTorch 中選擇 loss function。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch import nn\n",
    "\n",
    "torch.manual_seed(7)\n",
    "a = torch.randn(1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "print(a, b)\n",
    "lr = 1e-1\n",
    "epochs = 500\n",
    "\n",
    "# 指定一個 loss function\n",
    "MSELoss = nn.MSELoss(reduction='mean')\n",
    "\n",
    "\n",
    "optimizer = optim.SGD([a, b], lr=lr)\n",
    "for epoch in range(epochs):\n",
    "    yhat = a + b * x_train_tensor\n",
    "\n",
    "#-----------------------------------------\n",
    "    \n",
    "    # 不用再自己算 loss 了\n",
    "    # error = y_tensor - yhat\n",
    "    # loss = (error ** 2).mean()\n",
    "    loss = MSELoss(y_train_tensor, yhat)\n",
    "\n",
    "#-----------------------------------------\n",
    "    \n",
    "    # 不用再自己計算梯度了 \n",
    "    # a_grad = -2 * error.mean()\n",
    "    # b_grad = -2 * (x_tensor * error).mean()\n",
    "    # 從 loss 做 backward 來幫我們取得梯度\n",
    "    loss.backward()    \n",
    "    \n",
    "    # 不用手動更新梯度\n",
    "    # a -= lr * a.grad\n",
    "    # b -= lr * b.grad\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 不用自己將梯度清零\n",
    "    # a.grad.zero_()\n",
    "    # b.grad.zero_()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型（Model）\n",
    "1.在 PyTorch 中的 model 是用從 PyTorch Module 繼承來的 ,用Python class 表示的\n",
    "2.__init__(self)：定義了模型的組成。在我們的例子中，是參數a, b。當然我們也能定義一些參數之外的東西，像是層數之類的。\n",
    "3.forward(self, x)：執行計算，就是讓資料通過參數來得到現在的預測，不過使用的時候不是呼叫 forward() ，而是呼叫 model 本身，他就會輸出預測了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class PyTorchLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 使用 nn.Parameter 來表示 a, b 為參數\n",
    "        self.a = nn.Parameter(torch.randn(1, requires_grad=True))\n",
    "        self.b = nn.Parameter(torch.randn(1, requires_grad=True))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 計算我們的輸出，也就是預測\n",
    "        return self.a + self.b * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch import nn\n",
    "\n",
    "torch.manual_seed(7)\n",
    "model = PyTorchLinearRegression()#.to(device)\n",
    "\n",
    "print(model.state_dict())\n",
    "\n",
    "lr = 1e-1\n",
    "epochs = 500\n",
    "MSELoss = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "for epoch in range(epochs):\n",
    "#-----------------------------------------    \n",
    "    model.train() ##!!!!\n",
    "    \n",
    "    # 不用再手動做 output 了\n",
    "    # yhat = a + b * x_tensor\n",
    "    yhat = model(x_train_tensor)\n",
    "#-----------------------------------------\n",
    "\n",
    "    loss = MSELoss(y_train_tensor, yhat)\n",
    "    loss.backward()    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "這邊要注意的是，假如資料在 GPU 上，參數也必須在 GPU 上，這時就要用 to_device() 來把參數 tensor 也傳到 GPU 上。\n",
    "model = PyTorchLinearRegression().to(device)\n",
    "\n",
    "另外這裡我們也看到一段 model.train() 的程式碼，這段程式碼能夠讓我們將模型轉換到訓練模式，但並不會幫我們開始訓練模型，我們後面還是得定義訓練模型的步驟，後面會再解釋為何需要將模型轉為訓練模式。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch import nn\n",
    "\n",
    "torch.manual_seed(7)\n",
    "#model = PyTorchLinearRegression()#.to(device)\n",
    "\n",
    "model = nn.Sequential(nn.Linear(1, 1))\n",
    "\n",
    "print(model.state_dict())\n",
    "\n",
    "lr = 1e-1\n",
    "epochs = 500\n",
    "MSELoss = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "for epoch in range(epochs):\n",
    "#-----------------------------------------    \n",
    "    model.train() ##!!!!\n",
    "    \n",
    "    # 不用再手動做 output 了\n",
    "    # yhat = a + b * x_tensor\n",
    "    yhat = model(x_train_tensor)\n",
    "#-----------------------------------------\n",
    "\n",
    "    loss = MSELoss(y_train_tensor, yhat)\n",
    "    loss.backward()    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() will notify all your layers that you are in eval mode, that way, batchnorm or dropout layers will work in eval mode instead of training mode."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
